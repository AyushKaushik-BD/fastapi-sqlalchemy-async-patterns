---
title: "How Many Uvicorn Workers Do You Actually Need? (FastAPI Performance Guide)"
seoTitle: "How Many Uvicorn Workers? The FastAPI Performance Formula"
seoDescription: "Stop guessing your worker count. Learn the (2 x CPU) + 1 formula, avoid the Docker CPU trap, and optimize FastAPI concurrency for production."
datePublished: Wed Feb 04 2026 09:39:11 GMT+0000 (Coordinated Universal Time)
cuid: cml7u5kq3001802l5gdh940un
slug: how-many-uvicorn-workers-do-you-actually-need-fastapi-performance-guide
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1770197915325/3081bbb3-53f9-4d88-b35a-7a3aadf93c04.png
tags: backend, gunicorn, fastapi, task-management, uvicorn

---

You deploy your FastAPI app.

Docker builds successfully.  
Health checks pass.  
Traffic starts flowing.

Then comes the question every backend engineer eventually asks:

> **â€œHow many workers should I run?â€**

Pick a number too low, and your server sits idle while requests queue up.

Pick a number too high, and your machine spends more time juggling processes than executing code.

This is not a tuning detail.

**Worker count is one of the highest-impact performance decisions you make in production.**

Letâ€™s break down the math, kill a dangerous myth, and expose the container trap that silently destroys FastAPI performance.

## The Myth: "More Workers = More Speed"

It is natural to think: *"If 1 worker handles 1,000 requests, 10 workers will handle 10,000."*

**This assumption is dangerously wrong.**

Uvicorn workers are **processes**, not threads. Each worker is an independent instance of your Python application with its own memory space. If your server has **2 CPU Cores**, it can physically only run **2 things** at the exact same instant.

If you spawn **10 workers** on a **2-core** machine:

* The OS has to rapidly pause Worker #1 to let Worker #2 run, then pause #2 for #3, and so on.
    
* This "Context Switching" is expensive.
    
* You end up strictly *slower* than if you had just stuck to 2 or 3 workers.
    

More processes do not create more CPU, they only compete for it.

If your API slows down over time even with correct worker sizing, you may actually be leaking database sessions. I covered how to detect and fix that in my guide on [FastAPI session leaks](https://hashnode.com/post/cmlmm6nvn000h02la0y4c4ncx).

## The Worker Formula Most Engineers Get Wrong

For years, the Gunicorn documentation (which manages Uvicorn workers) has recommended a specific formula that strikes the perfect balance for Python web servers:

> **Workers = (2 x CPU Cores) + 1**

This formula is a starting point and not a law. Modern async workloads often need fewer workers than traditional WSGI apps.

### Why this number?

* **The "2x":** Even in an Async world, processes sometimes wait (System I/O, OS overhead). Having slightly more workers than cores ensures that if one worker is momentarily stuck doing non-async work, another creates a "pipeline" to keep the CPU busy.
    
* **The "+1":** This is the "spare tire." It handles the jitter/fluctuation to ensure 100% utilization.
    

### Examples:

* **1 Core (Standard AWS t3.micro):** `(2*1)+1` = **3 Workers**
    
* **2 Cores:** `(2*2)+1` = **5 Workers**
    
* **4 Cores:** `(2*4)+1` = **9 Workers**
    

*Checkout why* [*Uvicorn Health Checks Fail Under Load*](https://www.logiclooptech.dev/uvicorn-health-check-failure-kubernetes-fix) *and how to fix?*

## The "Memory" Constraint

The formula above assumes you have infinite RAM. **You do not.**

This is the #1 cause of crashing containers. Each Uvicorn worker loads your **entire** application into RAM.

* If your app takes **150MB** to boot (imports, ML models, caches).
    
* And you set `workers=10`.
    
* You just consumed **1.5 GB** of RAM immediately.
    

If your server only has 1GB of RAM, your container will crash with an **OOM (Out of Memory) Kill** before it serves a single request.

### The Revised Rule:

> **Calculate CPU limit first. Then check Memory limit.** If `(Workers * App Memory) > Total RAM`, **reduce workers.**

### When You Should BREAK the Formula?

Use fewer workers when:

* Your app is heavily async
    
* Most latency comes from network calls
    
* You use large DB pools
    
* Memory is tight
    

Use more workers when:

* You run CPU-heavy workloads
    
* You do image processing
    
* You hash aggressively
    
* You run ML inference
    

## The Docker Trap (Critical)

This is where 90% of deployments fail.

If you deploy to Kubernetes (K8s) or AWS ECS, you usually define a **CPU Limit** (e.g., `0.5 vCPU` or `2 vCPU`).

However, if you use Python's automatic detection inside the container:

Python

```plaintext
import multiprocessing
print(multiprocessing.cpu_count())
```

It often reports the **Physical Host's** CPU count (e.g., 64 Cores), not your container's limit (2 Cores).

**The Disaster Scenario:**

1. You are on a massive K8s Node (64 Cores).
    
2. Your pod has a limit of `2 CPUs`.
    
3. Your script sees 64 cores and spawns **129 workers** `(64*2 + 1)`.
    
4. Your 129 workers fight over 2 tiny CPU cores.
    
5. Performance creates a "Thundering Herd" problem, latency skyrockets, and health checks fail.
    

> **Containers lie about CPU. Always verify what your runtime actually sees.**

### The Fix

**Never** rely on auto-detection in containerized environments unless you are sure your library respects cgroups (limits). **Always** pass the worker count explicitly via an environment variable.

**In Dockerfile / Kubernetes:**

YAML

```plaintext
# CMD line
gunicorn -k uvicorn.workers.UvicornWorker -w $WORKERS main:app
```

**In Deployment YAML:**

YAML

```plaintext
env:
  - name: WORKERS
    value: "5"  # Manually calculated for 2 CPU limit
```

<div data-node-type="callout">
<div data-node-type="callout-emoji">ðŸ’¡</div>
<div data-node-type="callout-text">If your API is slow even after tuning workers, you are probably <a target="_self" rel="noopener noreferrer nofollow" href="https://www.logiclooptech.dev/why-fastapi-apps-slow-down-over-time-low-cpu-high-latency-explained" style="pointer-events: none">blocking the event loop</a>.</div>
</div>

## Running in Production: Gunicorn vs. Uvicorn

Should you run `uvicorn` directly?

Bash

```plaintext
# Development
uvicorn main:app --workers 4
```

For production, the standard is using **Gunicorn** as a process manager to spawn **Uvicorn** workers. Gunicorn is more robust at handling dead processes and signals.

Bash

```plaintext
# Production Standard
pip install gunicorn uvicorn
gunicorn -w 5 -k uvicorn.workers.UvicornWorker main:app
```

### Workers Do NOT Increase Concurrency

Async handles concurrency.

Workers provide **CPU isolation**, not scale.

If your app slows down under load, blindly increasing workers often makes things worse, especially when database pools get exhausted.

Fix architecture first.  
Then tune workers.

These are some FastAPI uvicorn workers best practices which you can follow to make your production alive and kicking.

## Conclusion: The Checklist

Don't guess. Calculate.

1. **Count your Cores:** Are you allocating 1 vCPU? 2 vCPUs?
    
2. **Apply the Formula:** `(2 * Cores) + 1`.
    
3. **Check Memory:** Multiply `Workers * 200MB`. Do you have enough RAM?
    
4. **Hardcode it:** Set the `WORKERS` env variable in your deployment config. Do not trust auto-detection.
    

> Donâ€™t guess your worker count.  
> Calculate it, or production will calculate it for you.

Once your async setup is correct, the next production bottleneck usually appears at the database layer specifically connection exhaustion. If you haven't hit it yet, you probably will.  
[**Fixing QueuePool Limit Errors in FastAPI**](https://www.logiclooptech.dev/fixing-queuepool-limit-reached-debugging-db-connection-leaks-in-fastapi)